---
title: "Apple Stock Price timeseries data ARIMA model fitting"
output:
  pdf_document: default
  html_document: default
  fig_width: 6 
  fig_height: 4 
---

Here we are doing a time-series analysis of the closing stock prices of the Apple stock for the last 5 years (from 2016 to 2020).

```{r include = FALSE}
library(zoo)
library(tidyr)
library(dplyr)
library(lubridate)
library(astsa)
library(forecast)
library(tseries)
library(quantmod)
library(tidyquant)
library(prophet)
library(ggplot2)
```

**getting the data**

```{r}
start <- as.Date("2016-01-01")
end <- as.Date("2020-12-31")
AAPL <- tq_get("AAPL", from = start, to = end)
```

**Viewing the data**

```{r}
head(AAPL)
```

Closing stock price vs. Date plot

```{r}
AAPL %>% ggplot(aes(x = date, y = close)) + geom_line()
```

The data shows clear trend and possibly some seasonality.

### Bollinger- Band plots

Moving average plot along with Moving Average +- standard deviation plots

1. Simple Moving Average (SMA; window = 50 days)

```{r}
# SMA
AAPL %>%
    ggplot(aes(x = date, y = close)) +
    geom_line() + 
    geom_bbands(aes(high = high, low = low, close = close), ma_fun = SMA, n = 50, linetype=5) +
    coord_x_date(xlim = c(start, end))
```

2. Exponential moving average (EMA; window = 50 days)

```{r}
# EMA
AAPL %>%
   ggplot(aes(x = date, y = close)) +
   geom_line() +           # Plot stock price
   geom_bbands(aes(high = high, low = low, close = close),
                  ma_fun = EMA, wilder = TRUE, ratio = NULL, n = 50) +
   coord_x_date(xlim = c(start, end))
```

3. Volume Weighted moving average (VWMA; window = 50 days)

```{r}
# VWMA
AAPL %>%
    ggplot(aes(x = date, y = close)) +
    geom_line() +           # Plot stock price
    geom_bbands(aes(high = high, low = low, close = close, volume = volume),
                   ma_fun = VWMA, n = 50) +
    coord_x_date(xlim = c(start,end))
```

Conducting ADF test for the Closing Price

```{r}
print(adf.test(AAPL$close))
```

The high p-value indicates that the data is non-stationary

### ACF and PACF plots

Let's first look at the auto-correlation function and the partial auto-correlation function plots.

```{r}
data <- AAPL$close

par(mfrow=c(2,1))
acf(data,main="Auto-Correlation Function of AAPL stock's Close Price",50)
pacf(data,main="Partial Auto-Correlation Function of AAPL stock's Close Price",50);
```

We see that the ACF plot shows lots of correlation for a very large number of lags.

## Guessing the right orders for ARIMA model fitting

1. Differncing orders d


```{r}
plot(diff(data),type="l")
```

So we got rid of the trend but some seasonality and a clear trend in the variation can be easily seen.

Let's do ADF test on the differenced data

```{r}
diff_data <- diff(data)
print(adf.test(diff_data))
```

The test confirms that the differenced data is (weakly) stationary.

2. orders for the auro-regressive (AR) and Moving Average (MA) terms i.e. p and q

**ACF and PACF for differenced data**

```{r}
par(mfrow=c(2,1))
acf(diff_data,main='differnced data ACF',50)
pacf(diff_data,main='differnced data PACF',50);
```

Now, the ACF plot does not show much correlation but PACF shows lots of correlation at several lags. But there is no abrupt drop and it's very difficult to guess the orders.

### Finding best parameters using
1. Grid Search

Trying for different values of p,q,P,Q and note down AIC, SSE and p-value (for Ljun-box-test). 
We want high p-values and small AIC and SSE using parsimony principle (simpler the better) while searching. Let's

```{r}

for(p in 1:5){
  for(d in 1:2){
    for(q in 1:5){
      if(p+d+q<=10){
      
        model<- arima(x=data, order = c(p-1,d,q-1))
          
        pval<-Box.test(model$residuals, lag=log(length(model$residuals)))
          
        sse<-sum(model$residuals^2)
          
        cat(p-1,d,q-1, 'AIC=', model$aic,' p-VALUE=', pval$p.value,'\n')
      }
    
    }
  }
}
```

2. Using auto.arima()

```{r}
#auto.arima( data, d = 1, D = 1,  max.p = 5,  max.q = 5,  max.P = 5,  max.Q = 5, max.order = 10,  start.p = 1,  start.q = 1,  start.P = 0, start.Q = 0, max.d=3, max.D=3, stationary = FALSE, seasonal = TRUE, ic="aic", stepwise = TRUE, approximation = FALSE)
model <- auto.arima(data, lambda = "auto")
model
```


### Best-model

The orders selected for the minimum AIC values (~4171) in the gird search method are 2,2,4 and 4,1,3. With auto.arima we found the order 4,1,1 correspnding to AIC~4205 which is only slightly large and has fewer parameters. All three models show siginificant p-values for the Ljung-Box test. We will proceed with the order 4,1,1 for the rest of the analysis.

### Train-test split

```{r}
N = length(data)
n = 0.7*N
train = data[1:n]
test  = data[(n+1):N]
```

### ARIMA(4,1,1) fitting results

```{r}
standard_residuals<- model$residuals/sd(model$residuals)
plot(standard_residuals,ylab='',main='Standardized Residuals')
```

```{r}
print(adf.test(standard_residuals))
```

We see that the residuals look almost stationary which we also confirmed with the ADF test

Let's check for correlations in the residual using the ACF plot

```{r}
acf(standard_residuals,50,main='ACF of standardized residuals');
```

The correlations at all lags seem to be insignificant for the residuals.

Next, we will perform a Ljung-Box test on the residuals. The null hypotheis for the test is:\
H0: The dataset points are independently distributed (not correlated).\
where a p-value of greater than 0.05 will be insifficient to reject the null hypothesis.

```{r}
for (lag in seq(1:50)){
  pval<-Box.test(model$residuals, lag=lag)
  p[lag]=pval$p.value
}
plot(p,ylim = (0.0:1), main='p-value from Ljung-Box test')
abline(h=0.05,lty=2)
```

Any value above the dashed line (at y=0.05) is significant. We see that the p-values of the Ljung-Box test at the lags < 17 are all significant and therefore the hypothesis that the residuals are not correlated cannot be rejected.

```{r}
pred_len=length(test)
plot(forecast(model, h=pred_len),main='Testing predictions')
train_x = seq(length(train)+1,length(train)+length(test))
lines(train_x,test)
```

Here the black line in the first left shows the training data. The blue line on the right showing the predictions from our model. The small shaded region on the blue lines which seem to cover the test data on the right completely shows the confidence interval of the predictions; it consists of two different dark and light shaded regions showing the 80% and 95% confidence regions.

### Forecasting using the best-model

```{r}
model<-arima(x=data, order = c(4,1,1))
par(mfrow=c(1,1))
h=30 # forecasting for the next 1 month after the end of the dataset
plot(forecast(model,h), main='Forecasts for next 1 months'); 
```

The uncertainty on the prediction seems to be pretty big for the ARIMA model.\
We will now use the *prophet* model for modeling the data and make predictions.

### Using Prophet for modeling

```{r}
df <- data.frame(ds = list(AAPL[,'date']), y = list(AAPL[,'close'])) %>% rename(ds=date,y=close)
head(df)
```

```{r}
len_train = nrow(df)*0.9
len_test = nrow(df) - len_train
df_train = df[1:len_train,]
df_test = df[len_train+1:nrow(df),]
m <- prophet(df_train)
future <- make_future_dataframe(m, periods = len_test)
forecast <- predict(m, future)
head(forecast[c('ds','yhat','yhat_lower','yhat_upper')])
```

```{r}
plot(m, forecast, xlabel = "date", ylabel = "stock close price ($)") + ggtitle("AAPL: Stock Price Prediction")

```

We see that the uncertainties in the predections obtained with the prophet model are quite small in comparison to the ARIMA model.
